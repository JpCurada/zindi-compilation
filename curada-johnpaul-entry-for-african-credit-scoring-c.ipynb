{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10423096,"sourceType":"datasetVersion","datasetId":6460401}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Can you predict the likelihood of a customer defaulting on a loan based on their financial data ?\n\nEntry of John Paul Curada for African Credit Scoring Challenge ","metadata":{}},{"cell_type":"markdown","source":"## I. Background\n\nThe African financial landscape is undergoing significant transformation with the rise of digital lending platforms and increased financial inclusion initiatives. However, assessing creditworthiness remains a complex challenge due to several unique factors:\n\nTraditional credit scoring methods often fall short in the African context because:\n1. Many potential borrowers lack conventional credit histories\n2. Economic conditions vary dramatically across different regions\n3. Market dynamics can shift rapidly, affecting borrowers' repayment capabilities\n4. Limited access to formal banking systems creates gaps in financial data\n\nA private asset manager operating across African financial markets has presented this challenge to develop more sophisticated approaches to credit risk assessment. The goal is to leverage machine learning to analyze both traditional loan data and macroeconomic indicators, creating a more comprehensive risk evaluation system.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## II. Objectives\n\nThe challenge has three primary objectives:\n\n1. **Model Development**\n   - Create a machine learning model that accurately predicts loan default probabilities\n   - Develop a solution that can generalize across different African markets\n   - Achieve high performance as measured by the F1 Score metric\n   - Balance model complexity with interpretability\n\n2. **Credit Scoring System**\n   - Design a scalable credit scoring function for the top 10 winners\n   - Transform model probabilities into actionable risk categories\n   - Create a system that can adapt to different market conditions\n   - Ensure the scoring system is transparent and implementable\n\n3. **Risk Assessment Framework**\n   - Incorporate both individual loan characteristics and economic indicators\n   - Identify key factors that contribute to default risk\n   - Create a framework that can support rapid decision-making\n   - Enable expansion into new markets while managing risk effectively\n\nSuccess in this challenge will contribute to more efficient lending practices across African markets, potentially increasing access to credit while maintaining sustainable risk levels for financial institutions.\n","metadata":{}},{"cell_type":"markdown","source":"## III. About Datasets\n\nThe analysis utilizes three primary datasets that provide comprehensive information about loan transactions, customer details, and economic indicators:\n\n1. **Train.csv/Test.csv**  \nThese files contain the core loan and customer information, consisting of 16 variables. The dataset includes unique identifiers, loan details (amount, type, duration), customer information, and lender-specific data. The target variable indicates whether a customer defaulted (1) or paid (0) their loan. This data forms the foundation for building our predictive model.\n\n| Variable Name | Description |\n|------------|-------------|\n| ID | A unique identifier for each entry in the dataset. |\n| customer_id | Unique identifier for each customer in the dataset. |\n| country_id | Identifier or code representing the country where the customer resides or where the loan was issued. |\n| tbl_loan_id | Unique identifier for each loan associated with the customer. |\n| Total_Amount | The total loan amount initially disbursed to the customer. |\n| Total_Amount_to_Repay | The total amount the customer is expected to repay, including principal, interest, and fees. |\n| loan_type | The category or type of loan. |\n| disbursement_date | The date when the loan amount was disbursed to the customer. |\n| duration | The length of the loan term, typically expressed in days |\n| lender_id | Unique identifier for the lender or institution that issued the loan. |\n| New_versus_Repeat | Indicates whether the loan is the customer's first loan (\"New\") or if the customer has taken loans before (\"Repeat\"). |\n| Amount_Funded_By_Lender | The portion of the loan funded directly by the lender. |\n| Lender_portion_Funded | Percentage of the total loan amount funded by the lender. |\n| due_date | The date by which the loan repayment is due. |\n| Lender_portion_to_be_repaid | The portion of the outstanding loan that needs to be repaid to the lender. |\n| target | This variables takes the value 0 or 1. 1 means the customer defaulted on the loan, whereas 0 means, the customer paid the loan. |\n\n2. **economic_indicators.csv**  \nThis dataset provides crucial macroeconomic context through nine key economic indicators for different countries. It includes:\n\n| Variable Name | Description |\n|------------|-------------|\n| FP.CPI.TOTL.ZG | Inflation, consumer prices (annual %) |\n| PA.NUS.FCRF | Official exchange rate (LCU per US$, period average) |\n| FR.INR.RINR | Real interest rate (%) |\n| AG.LND.PRCP.MM | Average precipitation in depth (mm per year) |\n| FR.INR.DPST | Deposit interest rate (%) |\n| FP.INR.LEND | Lending interest rate (%) |\n| FR.INR.LNDP | Interest rate spread (lending rate minus deposit rate, %) |\n| EG.USE.COMM.FO.ZS | Fossil fuel energy consumption (% of total) |\n| SL.UEM.TOTL.ZS | Unemployment rate |\n\nThese indicators can significantly influence a borrower's ability to repay loans, as they reflect the overall economic health of the country where the loan was issued.\n\nThe combination of individual loan data and macroeconomic indicators allows for a more comprehensive analysis of loan default risk, taking into account both personal financial circumstances and broader economic conditions that might affect loan repayment behavior.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Install Dependencies","metadata":{}},{"cell_type":"code","source":"#--------------------------------------\n# Install required packages\n#--------------------------------------\n!pip install exploralytics\n\n#--------------------------------------\n# Custom libraries\n#--------------------------------------\n# My own made open-source project/library for plotly viz\nfrom exploralytics import Visualizer \n\n#--------------------------------------\n# Data manipulation and analysis\n#--------------------------------------\nimport pandas as pd\nimport numpy as np\n\n#--------------------------------------\n# Visualization libraries\n#--------------------------------------\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n#--------------------------------------\n# Scikit-learn preprocessing\n#--------------------------------------\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import (\n   StandardScaler, \n   MinMaxScaler, \n   RobustScaler, \n   OneHotEncoder\n)\nfrom sklearn.impute import SimpleImputer, KNNImputer\n\n#--------------------------------------\n# Metrics and evaluation\n#--------------------------------------\nfrom sklearn.metrics import (\n   accuracy_score, \n   precision_score, \n   recall_score, \n   f1_score,\n   roc_curve, \n   precision_recall_curve, \n   auc, \n   roc_auc_score,\n   confusion_matrix, \n   classification_report\n)\nfrom sklearn.model_selection import cross_val_score\n\n#--------------------------------------\n# Machine Learning Models\n#--------------------------------------\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n#--------------------------------------\n# Configuration and Settings\n#--------------------------------------\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:56:44.404554Z","iopub.execute_input":"2025-01-11T13:56:44.404855Z","iopub.status.idle":"2025-01-11T13:56:55.800017Z","shell.execute_reply.started":"2025-01-11T13:56:44.404803Z","shell.execute_reply":"2025-01-11T13:56:55.799081Z"}},"outputs":[{"name":"stdout","text":"Collecting exploralytics\n  Downloading exploralytics-1.0.1-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from exploralytics) (2.1.4)\nRequirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from exploralytics) (5.24.1)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from exploralytics) (1.26.4)\nRequirement already satisfied: plotly-express>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from exploralytics) (0.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->exploralytics) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->exploralytics) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->exploralytics) (2024.1)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->exploralytics) (9.0.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->exploralytics) (24.1)\nRequirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from plotly-express>=0.4.0->exploralytics) (0.14.3)\nRequirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.10/dist-packages (from plotly-express>=0.4.0->exploralytics) (1.13.1)\nRequirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.10/dist-packages (from plotly-express>=0.4.0->exploralytics) (0.5.6)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5->plotly-express>=0.4.0->exploralytics) (1.16.0)\nDownloading exploralytics-1.0.1-py3-none-any.whl (12 kB)\nInstalling collected packages: exploralytics\nSuccessfully installed exploralytics-1.0.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/african-data-challenge/Train.csv')\ntest_df = pd.read_csv('/kaggle/input/african-data-challenge/Test.csv')\necon_in_df = pd.read_csv('/kaggle/input/african-data-challenge/economic_indicators.csv')\n\n# Display the first few rows of the datasets and their shape\ndisplay(f\"Train: {train_df.shape}\", train_df.head(),\n        f\"Test: {test_df.shape}\", test_df.head(),\n        f\"Economic Indicators: {econ_in_df.shape}\", econ_in_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:57:34.375059Z","iopub.execute_input":"2025-01-11T13:57:34.375472Z","iopub.status.idle":"2025-01-11T13:57:34.832715Z","shell.execute_reply.started":"2025-01-11T13:57:34.375438Z","shell.execute_reply":"2025-01-11T13:57:34.831860Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"'Train: (68654, 16)'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                      ID  customer_id country_id  tbl_loan_id  lender_id  \\\n0  ID_266671248032267278       266671      Kenya       248032     267278   \n1  ID_248919228515267278       248919      Kenya       228515     267278   \n2  ID_308486370501251804       308486      Kenya       370501     251804   \n3  ID_266004285009267278       266004      Kenya       285009     267278   \n4  ID_253803305312267278       253803      Kenya       305312     267278   \n\n  loan_type  Total_Amount  Total_Amount_to_Repay disbursement_date  \\\n0    Type_1        8448.0                 8448.0        2022-08-30   \n1    Type_1       25895.0                25979.0        2022-07-30   \n2    Type_7        6900.0                 7142.0        2024-09-06   \n3    Type_1        8958.0                 9233.0        2022-10-20   \n4    Type_1        4564.0                 4728.0        2022-11-28   \n\n     due_date  duration New_versus_Repeat  Amount_Funded_By_Lender  \\\n0  2022-09-06         7       Repeat Loan                   120.85   \n1  2022-08-06         7       Repeat Loan                  7768.50   \n2  2024-09-13         7       Repeat Loan                  1380.00   \n3  2022-10-27         7       Repeat Loan                  2687.40   \n4  2022-12-05         7       Repeat Loan                  1369.20   \n\n   Lender_portion_Funded  Lender_portion_to_be_repaid  target  \n0               0.014305                        121.0       0  \n1               0.300000                       7794.0       0  \n2               0.200000                       1428.0       0  \n3               0.300000                       2770.0       0  \n4               0.300000                       1418.0       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>customer_id</th>\n      <th>country_id</th>\n      <th>tbl_loan_id</th>\n      <th>lender_id</th>\n      <th>loan_type</th>\n      <th>Total_Amount</th>\n      <th>Total_Amount_to_Repay</th>\n      <th>disbursement_date</th>\n      <th>due_date</th>\n      <th>duration</th>\n      <th>New_versus_Repeat</th>\n      <th>Amount_Funded_By_Lender</th>\n      <th>Lender_portion_Funded</th>\n      <th>Lender_portion_to_be_repaid</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_266671248032267278</td>\n      <td>266671</td>\n      <td>Kenya</td>\n      <td>248032</td>\n      <td>267278</td>\n      <td>Type_1</td>\n      <td>8448.0</td>\n      <td>8448.0</td>\n      <td>2022-08-30</td>\n      <td>2022-09-06</td>\n      <td>7</td>\n      <td>Repeat Loan</td>\n      <td>120.85</td>\n      <td>0.014305</td>\n      <td>121.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_248919228515267278</td>\n      <td>248919</td>\n      <td>Kenya</td>\n      <td>228515</td>\n      <td>267278</td>\n      <td>Type_1</td>\n      <td>25895.0</td>\n      <td>25979.0</td>\n      <td>2022-07-30</td>\n      <td>2022-08-06</td>\n      <td>7</td>\n      <td>Repeat Loan</td>\n      <td>7768.50</td>\n      <td>0.300000</td>\n      <td>7794.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_308486370501251804</td>\n      <td>308486</td>\n      <td>Kenya</td>\n      <td>370501</td>\n      <td>251804</td>\n      <td>Type_7</td>\n      <td>6900.0</td>\n      <td>7142.0</td>\n      <td>2024-09-06</td>\n      <td>2024-09-13</td>\n      <td>7</td>\n      <td>Repeat Loan</td>\n      <td>1380.00</td>\n      <td>0.200000</td>\n      <td>1428.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_266004285009267278</td>\n      <td>266004</td>\n      <td>Kenya</td>\n      <td>285009</td>\n      <td>267278</td>\n      <td>Type_1</td>\n      <td>8958.0</td>\n      <td>9233.0</td>\n      <td>2022-10-20</td>\n      <td>2022-10-27</td>\n      <td>7</td>\n      <td>Repeat Loan</td>\n      <td>2687.40</td>\n      <td>0.300000</td>\n      <td>2770.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_253803305312267278</td>\n      <td>253803</td>\n      <td>Kenya</td>\n      <td>305312</td>\n      <td>267278</td>\n      <td>Type_1</td>\n      <td>4564.0</td>\n      <td>4728.0</td>\n      <td>2022-11-28</td>\n      <td>2022-12-05</td>\n      <td>7</td>\n      <td>Repeat Loan</td>\n      <td>1369.20</td>\n      <td>0.300000</td>\n      <td>1418.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Test: (18594, 15)'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                      ID  customer_id country_id  tbl_loan_id  lender_id  \\\n0  ID_269404226088267278       269404      Kenya       226088     267278   \n1  ID_255356300042267278       255356      Kenya       300042     267278   \n2  ID_257026243764267278       257026      Kenya       243764     267278   \n3  ID_264617299409267278       264617      Kenya       299409     267278   \n4  ID_247613296713267278       247613      Kenya       296713     267278   \n\n  loan_type  Total_Amount  Total_Amount_to_Repay disbursement_date  \\\n0    Type_1        1919.0                 1989.0        2022-07-27   \n1    Type_1        2138.0                 2153.0        2022-11-16   \n2    Type_1        8254.0                 8304.0        2022-08-24   \n3    Type_1        3379.0                 3379.0        2022-11-15   \n4    Type_1         120.0                  120.0        2022-11-10   \n\n     due_date  duration New_versus_Repeat  Amount_Funded_By_Lender  \\\n0  2022-08-03         7       Repeat Loan                    575.7   \n1  2022-11-23         7       Repeat Loan                      0.0   \n2  2022-08-31         7       Repeat Loan                    207.0   \n3  2022-11-22         7       Repeat Loan                   1013.7   \n4  2022-11-17         7       Repeat Loan                     36.0   \n\n   Lender_portion_Funded  Lender_portion_to_be_repaid  \n0               0.300000                        597.0  \n1               0.000000                          0.0  \n2               0.025079                        208.0  \n3               0.300000                       1014.0  \n4               0.300000                         36.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>customer_id</th>\n      <th>country_id</th>\n      <th>tbl_loan_id</th>\n      <th>lender_id</th>\n      <th>loan_type</th>\n      <th>Total_Amount</th>\n      <th>Total_Amount_to_Repay</th>\n      <th>disbursement_date</th>\n      <th>due_date</th>\n      <th>duration</th>\n      <th>New_versus_Repeat</th>\n      <th>Amount_Funded_By_Lender</th>\n      <th>Lender_portion_Funded</th>\n      <th>Lender_portion_to_be_repaid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_269404226088267278</td>\n      <td>269404</td>\n      <td>Kenya</td>\n      <td>226088</td>\n      <td>267278</td>\n      <td>Type_1</td>\n      <td>1919.0</td>\n      <td>1989.0</td>\n      <td>2022-07-27</td>\n      <td>2022-08-03</td>\n      <td>7</td>\n      <td>Repeat Loan</td>\n      <td>575.7</td>\n      <td>0.300000</td>\n      <td>597.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_255356300042267278</td>\n      <td>255356</td>\n      <td>Kenya</td>\n      <td>300042</td>\n      <td>267278</td>\n      <td>Type_1</td>\n      <td>2138.0</td>\n      <td>2153.0</td>\n      <td>2022-11-16</td>\n      <td>2022-11-23</td>\n      <td>7</td>\n      <td>Repeat Loan</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_257026243764267278</td>\n      <td>257026</td>\n      <td>Kenya</td>\n      <td>243764</td>\n      <td>267278</td>\n      <td>Type_1</td>\n      <td>8254.0</td>\n      <td>8304.0</td>\n      <td>2022-08-24</td>\n      <td>2022-08-31</td>\n      <td>7</td>\n      <td>Repeat Loan</td>\n      <td>207.0</td>\n      <td>0.025079</td>\n      <td>208.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_264617299409267278</td>\n      <td>264617</td>\n      <td>Kenya</td>\n      <td>299409</td>\n      <td>267278</td>\n      <td>Type_1</td>\n      <td>3379.0</td>\n      <td>3379.0</td>\n      <td>2022-11-15</td>\n      <td>2022-11-22</td>\n      <td>7</td>\n      <td>Repeat Loan</td>\n      <td>1013.7</td>\n      <td>0.300000</td>\n      <td>1014.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_247613296713267278</td>\n      <td>247613</td>\n      <td>Kenya</td>\n      <td>296713</td>\n      <td>267278</td>\n      <td>Type_1</td>\n      <td>120.0</td>\n      <td>120.0</td>\n      <td>2022-11-10</td>\n      <td>2022-11-17</td>\n      <td>7</td>\n      <td>Repeat Loan</td>\n      <td>36.0</td>\n      <td>0.300000</td>\n      <td>36.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Economic Indicators: (27, 25)'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         Country                                          Indicator  \\\n0          Ghana              Inflation, consumer prices (annual %)   \n1  Cote d'Ivoire              Inflation, consumer prices (annual %)   \n2          Kenya              Inflation, consumer prices (annual %)   \n3          Ghana  Official exchange rate (LCU per US$, period av...   \n4  Cote d'Ivoire  Official exchange rate (LCU per US$, period av...   \n\n       YR2001      YR2002      YR2003      YR2004      YR2005      YR2006  \\\n0   41.509496    9.360932   29.772980   18.042739   15.438992   11.679184   \n1    4.361529    3.077265    3.296807    1.457988    3.885830    2.467191   \n2    5.738598    1.961308    9.815691   11.624036   10.312778   14.453734   \n3    0.716305    0.792417    0.866764    0.899495    0.905209    0.915107   \n4  732.397693  693.713226  579.897426  527.338032  527.258363  522.425625   \n\n       YR2007      YR2008  ...      YR2014      YR2015      YR2016  \\\n0   10.734267   16.494640  ...   15.489616   17.149970   17.454635   \n1    1.892006    6.308528  ...    0.448682    1.251500    0.723178   \n2    9.758880   26.239817  ...    6.878155    6.582154    6.297250   \n3    0.932619    1.052275  ...    2.896575    3.714642    3.909817   \n4  478.633718  446.000041  ...  493.757330  591.211698  592.605615   \n\n       YR2017      YR2018      YR2019      YR2020      YR2021      YR2022  \\\n0   12.371922    7.808765    7.143640    9.887290    9.971089   31.255895   \n1    0.685881    0.359409   -1.106863    2.425007    4.091952    5.276167   \n2    8.005650    4.689806    5.239638    5.405162    6.107936    7.659863   \n3    4.350533    4.585325    5.217367    5.595708    5.805700    8.272400   \n4  580.656750  555.446458  585.911013  575.586005  554.530675  623.759701   \n\n       YR2023  \n0   38.106966  \n1    4.387117  \n2    7.671396  \n3   11.020408  \n4  606.569750  \n\n[5 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>Indicator</th>\n      <th>YR2001</th>\n      <th>YR2002</th>\n      <th>YR2003</th>\n      <th>YR2004</th>\n      <th>YR2005</th>\n      <th>YR2006</th>\n      <th>YR2007</th>\n      <th>YR2008</th>\n      <th>...</th>\n      <th>YR2014</th>\n      <th>YR2015</th>\n      <th>YR2016</th>\n      <th>YR2017</th>\n      <th>YR2018</th>\n      <th>YR2019</th>\n      <th>YR2020</th>\n      <th>YR2021</th>\n      <th>YR2022</th>\n      <th>YR2023</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ghana</td>\n      <td>Inflation, consumer prices (annual %)</td>\n      <td>41.509496</td>\n      <td>9.360932</td>\n      <td>29.772980</td>\n      <td>18.042739</td>\n      <td>15.438992</td>\n      <td>11.679184</td>\n      <td>10.734267</td>\n      <td>16.494640</td>\n      <td>...</td>\n      <td>15.489616</td>\n      <td>17.149970</td>\n      <td>17.454635</td>\n      <td>12.371922</td>\n      <td>7.808765</td>\n      <td>7.143640</td>\n      <td>9.887290</td>\n      <td>9.971089</td>\n      <td>31.255895</td>\n      <td>38.106966</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cote d'Ivoire</td>\n      <td>Inflation, consumer prices (annual %)</td>\n      <td>4.361529</td>\n      <td>3.077265</td>\n      <td>3.296807</td>\n      <td>1.457988</td>\n      <td>3.885830</td>\n      <td>2.467191</td>\n      <td>1.892006</td>\n      <td>6.308528</td>\n      <td>...</td>\n      <td>0.448682</td>\n      <td>1.251500</td>\n      <td>0.723178</td>\n      <td>0.685881</td>\n      <td>0.359409</td>\n      <td>-1.106863</td>\n      <td>2.425007</td>\n      <td>4.091952</td>\n      <td>5.276167</td>\n      <td>4.387117</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kenya</td>\n      <td>Inflation, consumer prices (annual %)</td>\n      <td>5.738598</td>\n      <td>1.961308</td>\n      <td>9.815691</td>\n      <td>11.624036</td>\n      <td>10.312778</td>\n      <td>14.453734</td>\n      <td>9.758880</td>\n      <td>26.239817</td>\n      <td>...</td>\n      <td>6.878155</td>\n      <td>6.582154</td>\n      <td>6.297250</td>\n      <td>8.005650</td>\n      <td>4.689806</td>\n      <td>5.239638</td>\n      <td>5.405162</td>\n      <td>6.107936</td>\n      <td>7.659863</td>\n      <td>7.671396</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ghana</td>\n      <td>Official exchange rate (LCU per US$, period av...</td>\n      <td>0.716305</td>\n      <td>0.792417</td>\n      <td>0.866764</td>\n      <td>0.899495</td>\n      <td>0.905209</td>\n      <td>0.915107</td>\n      <td>0.932619</td>\n      <td>1.052275</td>\n      <td>...</td>\n      <td>2.896575</td>\n      <td>3.714642</td>\n      <td>3.909817</td>\n      <td>4.350533</td>\n      <td>4.585325</td>\n      <td>5.217367</td>\n      <td>5.595708</td>\n      <td>5.805700</td>\n      <td>8.272400</td>\n      <td>11.020408</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cote d'Ivoire</td>\n      <td>Official exchange rate (LCU per US$, period av...</td>\n      <td>732.397693</td>\n      <td>693.713226</td>\n      <td>579.897426</td>\n      <td>527.338032</td>\n      <td>527.258363</td>\n      <td>522.425625</td>\n      <td>478.633718</td>\n      <td>446.000041</td>\n      <td>...</td>\n      <td>493.757330</td>\n      <td>591.211698</td>\n      <td>592.605615</td>\n      <td>580.656750</td>\n      <td>555.446458</td>\n      <td>585.911013</td>\n      <td>575.586005</td>\n      <td>554.530675</td>\n      <td>623.759701</td>\n      <td>606.569750</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 25 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## IV.Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"### 1. Data Quality Checks\n\n","metadata":{}},{"cell_type":"markdown","source":"#### a. Missing Values","metadata":{}},{"cell_type":"code","source":"def print_missing_summary(df, df_name=None):\n   \"\"\"\n   Prints missing values summary with counts and percentages.\n   \"\"\"\n   total_rows = len(df)\n   # Use df_name if provided, otherwise try to get the variable name\n   name = df_name or getattr(df, '__name__', 'DataFrame')\n   print(f\"{name}\")\n   print(f\"Total rows: {total_rows}\")\n   print(\"\\nMissing Values Summary: \")\n   print(\"-\" * 50)\n   max_col_length = max(len(str(col)) for col in df.columns)\n   for col in df.columns:\n       missing = df[col].isnull().sum()\n       percent = (missing/total_rows * 100).round(2)\n       print(f\"{str(col):<{max_col_length}}\\t\\t{missing} missing ({percent}%)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T14:18:07.478375Z","iopub.execute_input":"2025-01-11T14:18:07.478692Z","iopub.status.idle":"2025-01-11T14:18:07.484548Z","shell.execute_reply.started":"2025-01-11T14:18:07.478663Z","shell.execute_reply":"2025-01-11T14:18:07.483486Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print_missing_summary(train_df, \"Training data (train_df)\")\nprint_missing_summary(test_df, \"Testing data (test_df)\")\nprint_missing_summary(econ_in_df, \"Economic Indicator (econ_in_df\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T14:18:56.287552Z","iopub.execute_input":"2025-01-11T14:18:56.287926Z","iopub.status.idle":"2025-01-11T14:18:56.341317Z","shell.execute_reply.started":"2025-01-11T14:18:56.287856Z","shell.execute_reply":"2025-01-11T14:18:56.340438Z"}},"outputs":[{"name":"stdout","text":"Training data (train_df)\nTotal rows: 68654\n\nMissing Values Summary: \n--------------------------------------------------\nID                         \t\t0 missing (0.0%)\ncustomer_id                \t\t0 missing (0.0%)\ncountry_id                 \t\t0 missing (0.0%)\ntbl_loan_id                \t\t0 missing (0.0%)\nlender_id                  \t\t0 missing (0.0%)\nloan_type                  \t\t0 missing (0.0%)\nTotal_Amount               \t\t0 missing (0.0%)\nTotal_Amount_to_Repay      \t\t0 missing (0.0%)\ndisbursement_date          \t\t0 missing (0.0%)\ndue_date                   \t\t0 missing (0.0%)\nduration                   \t\t0 missing (0.0%)\nNew_versus_Repeat          \t\t0 missing (0.0%)\nAmount_Funded_By_Lender    \t\t0 missing (0.0%)\nLender_portion_Funded      \t\t0 missing (0.0%)\nLender_portion_to_be_repaid\t\t0 missing (0.0%)\ntarget                     \t\t0 missing (0.0%)\nTesting data (test_df)\nTotal rows: 18594\n\nMissing Values Summary: \n--------------------------------------------------\nID                         \t\t0 missing (0.0%)\ncustomer_id                \t\t0 missing (0.0%)\ncountry_id                 \t\t0 missing (0.0%)\ntbl_loan_id                \t\t0 missing (0.0%)\nlender_id                  \t\t0 missing (0.0%)\nloan_type                  \t\t0 missing (0.0%)\nTotal_Amount               \t\t0 missing (0.0%)\nTotal_Amount_to_Repay      \t\t0 missing (0.0%)\ndisbursement_date          \t\t0 missing (0.0%)\ndue_date                   \t\t0 missing (0.0%)\nduration                   \t\t0 missing (0.0%)\nNew_versus_Repeat          \t\t0 missing (0.0%)\nAmount_Funded_By_Lender    \t\t0 missing (0.0%)\nLender_portion_Funded      \t\t0 missing (0.0%)\nLender_portion_to_be_repaid\t\t0 missing (0.0%)\nEconomic Indicator (econ_in_df\nTotal rows: 27\n\nMissing Values Summary: \n--------------------------------------------------\nCountry  \t\t0 missing (0.0%)\nIndicator\t\t0 missing (0.0%)\nYR2001   \t\t7 missing (25.93%)\nYR2002   \t\t7 missing (25.93%)\nYR2003   \t\t7 missing (25.93%)\nYR2004   \t\t7 missing (25.93%)\nYR2005   \t\t3 missing (11.11%)\nYR2006   \t\t3 missing (11.11%)\nYR2007   \t\t3 missing (11.11%)\nYR2008   \t\t3 missing (11.11%)\nYR2009   \t\t3 missing (11.11%)\nYR2010   \t\t3 missing (11.11%)\nYR2011   \t\t3 missing (11.11%)\nYR2012   \t\t3 missing (11.11%)\nYR2013   \t\t3 missing (11.11%)\nYR2014   \t\t3 missing (11.11%)\nYR2015   \t\t6 missing (22.22%)\nYR2016   \t\t6 missing (22.22%)\nYR2017   \t\t6 missing (22.22%)\nYR2018   \t\t10 missing (37.04%)\nYR2019   \t\t10 missing (37.04%)\nYR2020   \t\t10 missing (37.04%)\nYR2021   \t\t10 missing (37.04%)\nYR2022   \t\t13 missing (48.15%)\nYR2023   \t\t13 missing (48.15%)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### b. Data Types","metadata":{}},{"cell_type":"code","source":"def convert_datatypes(df):\n   \"\"\"\n   Convert DataFrame columns to appropriate data types\n   \"\"\"\n   # Create a copy to avoid modifying original\n   df = df.copy()\n\n   # Convert ID columns to string/object\n   id_columns = ['ID', 'country_id', 'customer_id','tbl_loan_id']\n   df[id_columns] = df[id_columns].astype(str)\n\n   # Convert numeric ID columns to int64\n   numeric_id_columns = ['duration']  # Removed customer_id\n   df[numeric_id_columns] = df[numeric_id_columns].astype('int64')\n\n   # Convert float columns\n   float_columns = [\n       'Total_Amount',\n       'Total_Amount_to_Repay',\n       'Amount_Funded_By_Lender',\n       'Lender_portion_Funded',\n       'Lender_portion_to_be_repaid'\n   ]\n   df[float_columns] = df[float_columns].astype('float64')\n\n   # Convert date columns to datetime\n   date_columns = ['disbursement_date', 'due_date']\n   df[date_columns] = df[date_columns].apply(pd.to_datetime)\n\n   # Convert categorical columns\n   df['loan_type'] = df['loan_type'].astype('category')\n   df['New_versus_Repeat'] = df['New_versus_Repeat'].astype('category')\n   df['lender_id'] = df['lender_id'].astype('category')\n\n   return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T14:19:23.808604Z","iopub.execute_input":"2025-01-11T14:19:23.808992Z","iopub.status.idle":"2025-01-11T14:19:23.815292Z","shell.execute_reply.started":"2025-01-11T14:19:23.808957Z","shell.execute_reply":"2025-01-11T14:19:23.814236Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Apply the conversion\ntrain_df = convert_datatypes(train_df)\n\n# Verify the changes\nprint(\"Dataset Info After Type Conversion:\")\nprint(train_df.info())\n\n# Memory usage before and after\nprint(\"\\nMemory Usage:\")\nprint(f\"Memory usage: {train_df.memory_usage().sum() / 1024**2:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T14:19:50.514445Z","iopub.execute_input":"2025-01-11T14:19:50.514828Z","iopub.status.idle":"2025-01-11T14:19:50.667920Z","shell.execute_reply.started":"2025-01-11T14:19:50.514797Z","shell.execute_reply":"2025-01-11T14:19:50.667120Z"}},"outputs":[{"name":"stdout","text":"Dataset Info After Type Conversion:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 68654 entries, 0 to 68653\nData columns (total 16 columns):\n #   Column                       Non-Null Count  Dtype         \n---  ------                       --------------  -----         \n 0   ID                           68654 non-null  object        \n 1   customer_id                  68654 non-null  object        \n 2   country_id                   68654 non-null  object        \n 3   tbl_loan_id                  68654 non-null  object        \n 4   lender_id                    68654 non-null  category      \n 5   loan_type                    68654 non-null  category      \n 6   Total_Amount                 68654 non-null  float64       \n 7   Total_Amount_to_Repay        68654 non-null  float64       \n 8   disbursement_date            68654 non-null  datetime64[ns]\n 9   due_date                     68654 non-null  datetime64[ns]\n 10  duration                     68654 non-null  int64         \n 11  New_versus_Repeat            68654 non-null  category      \n 12  Amount_Funded_By_Lender      68654 non-null  float64       \n 13  Lender_portion_Funded        68654 non-null  float64       \n 14  Lender_portion_to_be_repaid  68654 non-null  float64       \n 15  target                       68654 non-null  int64         \ndtypes: category(3), datetime64[ns](2), float64(5), int64(2), object(4)\nmemory usage: 7.0+ MB\nNone\n\nMemory Usage:\nMemory usage: 7.01 MB\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### c. Basic Statistics","metadata":{}},{"cell_type":"code","source":"def print_readable_stats(df):\n    \"\"\"\n    Print formatted descriptive statistics including median\n    \"\"\"\n    # Select only numeric columns\n    numeric_df = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32'])\n\n    stats = numeric_df.describe()\n\n    # Format numbers with comma separators\n    formatted_stats = pd.DataFrame({\n        'Count': stats.loc['count'].map('{:,.0f}'.format),\n        'Mean': stats.loc['mean'].map('{:,.2f}'.format),\n        'Median': numeric_df.median().map('{:,.2f}'.format),\n        'Std': stats.loc['std'].map('{:,.2f}'.format),\n        'Min': stats.loc['min'].map('{:,.2f}'.format),\n        '25%': stats.loc['25%'].map('{:,.2f}'.format),\n        '75%': stats.loc['75%'].map('{:,.2f}'.format),\n        'Max': stats.loc['max'].map('{:,.2f}'.format)\n    })\n\n    # Print with clean formatting\n    print(\"\\nDescriptive Statistics for Numeric Columns:\")\n    print(\"----------------------------------------\")\n    print(formatted_stats.to_string())\n\n    # Additional info per column\n    print(\"\\nDetailed Column Information:\")\n    print(\"--------------------------\")\n    for col in numeric_df.columns:\n        print(f\"\\n{col}:\")\n        print(f\"Skewness: {numeric_df[col].skew():.2f}\")\n        print(f\"Missing Values: {numeric_df[col].isnull().sum():,} ({numeric_df[col].isnull().mean()*100:.1f}%)\")\n\n\n# Usage\nprint_readable_stats(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T14:20:17.507918Z","iopub.execute_input":"2025-01-11T14:20:17.508303Z","iopub.status.idle":"2025-01-11T14:20:17.576116Z","shell.execute_reply.started":"2025-01-11T14:20:17.508268Z","shell.execute_reply":"2025-01-11T14:20:17.575315Z"}},"outputs":[{"name":"stdout","text":"\nDescriptive Statistics for Numeric Columns:\n----------------------------------------\n                              Count       Mean    Median         Std   Min       25%        75%            Max\nTotal_Amount                 68,654  14,836.83  5,249.00  141,649.87  2.00  2,295.00  11,450.00  23,000,000.00\nTotal_Amount_to_Repay        68,654  15,639.93  5,325.00  165,078.35  0.00  2,329.00  11,650.00  25,415,000.00\nduration                     68,654       8.54      7.00       13.34  1.00      7.00       7.00       1,096.00\nAmount_Funded_By_Lender      68,654   2,545.66    915.00   11,922.72  0.00    234.00   2,272.65   1,600,000.00\nLender_portion_Funded        68,654       0.22      0.30        0.13  0.00      0.12       0.30           1.17\nLender_portion_to_be_repaid  68,654   2,652.62    934.00   13,380.06  0.00    239.00   2,317.00   1,821,338.00\ntarget                       68,654       0.02      0.00        0.13  0.00      0.00       0.00           1.00\n\nDetailed Column Information:\n--------------------------\n\nTotal_Amount:\nSkewness: 103.83\nMissing Values: 0 (0.0%)\n\nTotal_Amount_to_Repay:\nSkewness: 104.58\nMissing Values: 0 (0.0%)\n\nduration:\nSkewness: 34.06\nMissing Values: 0 (0.0%)\n\nAmount_Funded_By_Lender:\nSkewness: 61.06\nMissing Values: 0 (0.0%)\n\nLender_portion_Funded:\nSkewness: -0.21\nMissing Values: 0 (0.0%)\n\nLender_portion_to_be_repaid:\nSkewness: 63.20\nMissing Values: 0 (0.0%)\n\ntarget:\nSkewness: 7.18\nMissing Values: 0 (0.0%)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"we might NOT need to remove outliers because\n- Extreme values in loan amounts could be legitimate high-value loans\n- Longer durations might indicate different loan products\n- Outliers might actually be predictive of default risk\n- Removing them could hide important risk patterns","metadata":{}},{"cell_type":"code","source":"def plot_target_distribution(df, target):\n   \"\"\"\n   Plots target class distribution with counts and percentages.\n   Colors bars based on value - green for high, red for low.\n   \"\"\"\n   # Get value counts and percentages\n   counts = df[target].value_counts()\n   percentages = df[target].value_counts(normalize=True) * 100\n\n   # Create figure with count and percentage bars\n   fig = go.Figure()\n\n   # Set colors based on count values\n   colors = ['#03EF62' if count == counts.max() else '#FF999C'\n            for count in [counts[0], counts[1]]]\n\n   # Add count bars\n   fig.add_trace(go.Bar(\n       x=[0, 1],  # Use actual values instead of unique method\n       y=[counts[0], counts[1]],\n       text=[f'n={counts[0]}<br>{percentages[0]:.1f}%',\n             f'n={counts[1]}<br>{percentages[1]:.1f}%'],\n       textposition='auto',\n       name='Count',\n       marker_color=colors\n   ))\n\n   # Update layout\n   fig.update_layout(\n       title='Target Distribution (0: Paid, 1: Default)',\n       xaxis_title='Status',\n       yaxis_title='Count',\n       showlegend=False\n   )\n\n   fig.show()\n\n   # Print summary\n   print(f\"\\nClass Distribution Summary:\")\n   print(\"-\" * 30)\n   print(f\"Paid (0): {counts[0]} ({percentages[0]:.1f}%)\")\n   print(f\"Default (1): {counts[1]} ({percentages[1]:.1f}%)\")\n\n# Example usage\nplot_target_distribution(train_df, 'target')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T14:20:48.090756Z","iopub.execute_input":"2025-01-11T14:20:48.091133Z","iopub.status.idle":"2025-01-11T14:20:48.376138Z","shell.execute_reply.started":"2025-01-11T14:20:48.091105Z","shell.execute_reply":"2025-01-11T14:20:48.375268Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"5d15c87f-cef4-4b29-8f88-20160135b650\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5d15c87f-cef4-4b29-8f88-20160135b650\")) {                    Plotly.newPlot(                        \"5d15c87f-cef4-4b29-8f88-20160135b650\",                        [{\"marker\":{\"color\":[\"#03EF62\",\"#FF999C\"]},\"name\":\"Count\",\"text\":[\"n=67396\\u003cbr\\u003e98.2%\",\"n=1258\\u003cbr\\u003e1.8%\"],\"textposition\":\"auto\",\"x\":[0,1],\"y\":[67396,1258],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Target Distribution (0: Paid, 1: Default)\"},\"xaxis\":{\"title\":{\"text\":\"Status\"}},\"yaxis\":{\"title\":{\"text\":\"Count\"}},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('5d15c87f-cef4-4b29-8f88-20160135b650');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                            </script>        </div>\n</body>\n</html>"},"metadata":{}},{"name":"stdout","text":"\nClass Distribution Summary:\n------------------------------\nPaid (0): 67396 (98.2%)\nDefault (1): 1258 (1.8%)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### Basic Exploration","metadata":{}},{"cell_type":"code","source":"def explore_datasets(train_df, test_df, econ_df):\n    # 1. Customer Analysis\n    train_customers = train_df['customer_id'].nunique()\n    test_customers = test_df['customer_id'].nunique()\n    common_customers = len(set(train_df['customer_id']).intersection(set(test_df['customer_id'])))\n    \n    print(\"Customer Analysis:\")\n    print(f\"Train customers: {train_customers}\")\n    print(f\"Test customers: {test_customers}\")\n    print(f\"Common customers: {common_customers}\")\n    \n    # 2. Basic Dataset Properties\n    print(\"\\nDataset Sizes:\")\n    print(f\"Train shape: {train_df.shape}\")\n    print(f\"Test shape: {test_df.shape}\")\n    print(f\"Economic indicators shape: {econ_df.shape}\")\n    \n    # 3. Loan Type Distribution\n    print(\"\\nLoan Type Distribution (Train):\")\n    print(train_df['loan_type'].value_counts(normalize=True).round(3))\n    \n    # 4. New vs Repeat Borrowers\n    print(\"\\nNew vs Repeat Borrowers (Train):\")\n    print(train_df['New_versus_Repeat'].value_counts(normalize=True).round(3))\n    \n    # 5. Default Rate Analysis\n    print(\"\\nDefault Rate Analysis (Train):\")\n    default_rate = train_df['target'].mean()\n    print(f\"Overall default rate: {default_rate:.2%}\")\n    \n    # 6. Country Analysis\n    print(\"\\nCountry Distribution (Train):\")\n    print(train_df['country_id'].value_counts(normalize=True).round(3))\n    \n    # 7. Basic Loan Amount Statistics\n    print(\"\\nLoan Amount Statistics (Train):\")\n    amount_stats = train_df['Total_Amount'].describe()\n    print(amount_stats)\n    \n    # 8. Duration Analysis\n    print(\"\\nLoan Duration Statistics (Train):\")\n    duration_stats = train_df['duration'].describe()\n    print(duration_stats)\n    \n    # 9. Economic Indicators Coverage\n    print(\"\\nEconomic Indicators Coverage:\")\n    missing_econ = econ_df.isnull().sum() / len(econ_df)\n    print(missing_econ.round(3))\n\n    return {\n        'train_customers': train_customers,\n        'test_customers': test_customers,\n        'common_customers': common_customers,\n        'default_rate': default_rate\n    }\n\n# Usage:\nstats = explore_datasets(train_df, test_df, econ_in_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T14:25:39.284301Z","iopub.execute_input":"2025-01-11T14:25:39.284660Z","iopub.status.idle":"2025-01-11T14:25:39.336531Z","shell.execute_reply.started":"2025-01-11T14:25:39.284621Z","shell.execute_reply":"2025-01-11T14:25:39.335434Z"}},"outputs":[{"name":"stdout","text":"Customer Analysis:\nTrain customers: 6540\nTest customers: 4962\nCommon customers: 0\n\nDataset Sizes:\nTrain shape: (68654, 16)\nTest shape: (18594, 15)\nEconomic indicators shape: (27, 25)\n\nLoan Type Distribution (Train):\nloan_type\nType_1     0.899\nType_7     0.041\nType_5     0.022\nType_4     0.018\nType_10    0.007\nType_6     0.005\nType_9     0.003\nType_14    0.001\nType_2     0.001\nType_11    0.001\nType_18    0.000\nType_17    0.000\nType_12    0.000\nType_23    0.000\nType_20    0.000\nType_16    0.000\nType_13    0.000\nType_19    0.000\nType_15    0.000\nType_21    0.000\nType_24    0.000\nType_22    0.000\nName: proportion, dtype: float64\n\nNew vs Repeat Borrowers (Train):\nNew_versus_Repeat\nRepeat Loan    0.992\nNew Loan       0.008\nName: proportion, dtype: float64\n\nDefault Rate Analysis (Train):\nOverall default rate: 1.83%\n\nCountry Distribution (Train):\ncountry_id\nKenya    1.0\nName: proportion, dtype: float64\n\nLoan Amount Statistics (Train):\ncount    6.865400e+04\nmean     1.483683e+04\nstd      1.416499e+05\nmin      2.000000e+00\n25%      2.295000e+03\n50%      5.249000e+03\n75%      1.145000e+04\nmax      2.300000e+07\nName: Total_Amount, dtype: float64\n\nLoan Duration Statistics (Train):\ncount    68654.000000\nmean         8.544586\nstd         13.343145\nmin          1.000000\n25%          7.000000\n50%          7.000000\n75%          7.000000\nmax       1096.000000\nName: duration, dtype: float64\n\nEconomic Indicators Coverage:\nCountry      0.000\nIndicator    0.000\nYR2001       0.259\nYR2002       0.259\nYR2003       0.259\nYR2004       0.259\nYR2005       0.111\nYR2006       0.111\nYR2007       0.111\nYR2008       0.111\nYR2009       0.111\nYR2010       0.111\nYR2011       0.111\nYR2012       0.111\nYR2013       0.111\nYR2014       0.111\nYR2015       0.222\nYR2016       0.222\nYR2017       0.222\nYR2018       0.370\nYR2019       0.370\nYR2020       0.370\nYR2021       0.370\nYR2022       0.481\nYR2023       0.481\ndtype: float64\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def analyze_ids_and_loans(train_df, test_df):\n    \"\"\"\n    Analyze IDs and loan types in the datasets\n    \"\"\"\n    print(\"ID Analysis:\")\n    print(\"============\")\n    \n    # Customer ID Analysis\n    train_customers = train_df['customer_id'].nunique()\n    test_customers = test_df['customer_id'].nunique()\n    common_customers = len(set(train_df['customer_id']).intersection(set(test_df['customer_id'])))\n    \n    print(f\"\\nCustomer IDs:\")\n    print(f\"Train unique customers: {train_customers:,}\")\n    print(f\"Test unique customers: {test_customers:,}\")\n    print(f\"Common customers: {common_customers:,}\")\n    print(f\"Customer overlap: {(common_customers/train_customers*100):.2f}%\")\n    \n    # Loan ID Analysis\n    print(f\"\\nLoan IDs (tbl_loan_id):\")\n    print(f\"Train unique loans: {train_df['tbl_loan_id'].nunique():,}\")\n    print(f\"Test unique loans: {test_df['tbl_loan_id'].nunique():,}\")\n    common_loans = len(set(train_df['tbl_loan_id']).intersection(set(test_df['tbl_loan_id'])))\n    print(f\"Common loans: {common_loans:,}\")\n    \n    # Lender ID Analysis\n    print(f\"\\nLender IDs:\")\n    print(f\"Train unique lenders: {train_df['lender_id'].nunique():,}\")\n    print(f\"Test unique lenders: {test_df['lender_id'].nunique():,}\")\n    common_lenders = len(set(train_df['lender_id']).intersection(set(test_df['lender_id'])))\n    print(f\"Common lenders: {common_lenders:,}\")\n    \n    print(\"\\nLoan Type Analysis:\")\n    print(\"==================\")\n    \n    # Loan Type Distribution\n    print(\"\\nLoan Type Distribution in Train:\")\n    loan_counts = train_df['loan_type'].value_counts()\n    loan_percentages = train_df['loan_type'].value_counts(normalize=True) * 100\n    \n    for loan_type, count in loan_counts.items():\n        print(f\"{loan_type}: {count:,} ({loan_percentages[loan_type]:.2f}%)\")\n    \n    # Default Rates by Loan Type\n    print(\"\\nDefault Rates by Loan Type:\")\n    default_rates = train_df.groupby('loan_type')['target'].mean() * 100\n    for loan_type, rate in default_rates.items():\n        print(f\"{loan_type}: {rate:.2f}%\")\n        \n    # Check for loan types present in test but not in train\n    test_only_types = set(test_df['loan_type']) - set(train_df['loan_type'])\n    if test_only_types:\n        print(\"\\nWARNING: Loan types in test but not in train:\")\n        print(test_only_types)\n\n    return {\n        'train_customers': train_customers,\n        'test_customers': test_customers,\n        'common_customers': common_customers,\n        'loan_types': loan_counts.to_dict(),\n        'default_rates': default_rates.to_dict()\n    }\n\n# Usage:\nstats = analyze_ids_and_loans(train_df, test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T14:27:57.044353Z","iopub.execute_input":"2025-01-11T14:27:57.044673Z","iopub.status.idle":"2025-01-11T14:27:57.120659Z","shell.execute_reply.started":"2025-01-11T14:27:57.044647Z","shell.execute_reply":"2025-01-11T14:27:57.119676Z"}},"outputs":[{"name":"stdout","text":"ID Analysis:\n============\n\nCustomer IDs:\nTrain unique customers: 6,540\nTest unique customers: 4,962\nCommon customers: 0\nCustomer overlap: 0.00%\n\nLoan IDs (tbl_loan_id):\nTrain unique loans: 66,520\nTest unique loans: 17,067\nCommon loans: 0\n\nLender IDs:\nTrain unique lenders: 4\nTest unique lenders: 8\nCommon lenders: 4\n\nLoan Type Analysis:\n==================\n\nLoan Type Distribution in Train:\nType_1: 61,723 (89.90%)\nType_7: 2,790 (4.06%)\nType_5: 1,521 (2.22%)\nType_4: 1,235 (1.80%)\nType_10: 466 (0.68%)\nType_6: 357 (0.52%)\nType_9: 205 (0.30%)\nType_14: 99 (0.14%)\nType_2: 74 (0.11%)\nType_11: 42 (0.06%)\nType_18: 30 (0.04%)\nType_17: 23 (0.03%)\nType_12: 21 (0.03%)\nType_23: 18 (0.03%)\nType_20: 13 (0.02%)\nType_16: 12 (0.02%)\nType_13: 10 (0.01%)\nType_19: 4 (0.01%)\nType_15: 4 (0.01%)\nType_21: 3 (0.00%)\nType_24: 3 (0.00%)\nType_22: 1 (0.00%)\n\nDefault Rates by Loan Type:\nType_1: 0.84%\nType_10: 2.58%\nType_11: 4.76%\nType_12: 4.76%\nType_13: 0.00%\nType_14: 71.72%\nType_15: 75.00%\nType_16: 0.00%\nType_17: 0.00%\nType_18: 3.33%\nType_19: 0.00%\nType_2: 60.81%\nType_20: 53.85%\nType_21: 0.00%\nType_22: 0.00%\nType_23: 72.22%\nType_24: 0.00%\nType_4: 12.31%\nType_5: 8.02%\nType_6: 10.64%\nType_7: 8.42%\nType_9: 17.56%\n\nWARNING: Loan types in test but not in train:\n{'Type_3', 'Type_8'}\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Check percentage of test data affected\naffected_pct = (test_df['loan_type'].isin(['Type_3', 'Type_8']).mean() * 100)\nprint(f\"Percentage of test data affected: {affected_pct:.2f}%\")\n\n# Check loan_type importance in training\nfrom sklearn.metrics import mutual_info_score\nmi_score = mutual_info_score(train_df['loan_type'], train_df['target'])\nprint(f\"Mutual information with target: {mi_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T14:29:38.683012Z","iopub.execute_input":"2025-01-11T14:29:38.683332Z","iopub.status.idle":"2025-01-11T14:29:38.777577Z","shell.execute_reply.started":"2025-01-11T14:29:38.683309Z","shell.execute_reply":"2025-01-11T14:29:38.776509Z"}},"outputs":[{"name":"stdout","text":"Percentage of test data affected: 16.54%\nMutual information with target: 0.0170\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"### d.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## V. Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:18.762628Z","iopub.status.idle":"2025-01-11T12:26:18.763026Z","shell.execute_reply":"2025-01-11T12:26:18.762860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_datatypes(df):\n   \"\"\"\n   Convert DataFrame columns to appropriate data types\n   \"\"\"\n   # Create a copy to avoid modifying original\n   df = df.copy()\n\n   # Convert ID columns to string/object\n   id_columns = ['ID', 'country_id', 'customer_id', 'lender_id']\n   df[id_columns] = df[id_columns].astype(str)\n\n   # Convert numeric ID columns to int64\n   numeric_id_columns = ['tbl_loan_id']  # Removed customer_id\n   df[numeric_id_columns] = df[numeric_id_columns].astype('int64')\n\n   # Convert float columns\n   float_columns = [\n       'Total_Amount',\n       'Total_Amount_to_Repay',\n       'Amount_Funded_By_Lender',\n       'Lender_portion_Funded',\n       'Lender_portion_to_be_repaid'\n   ]\n   df[float_columns] = df[float_columns].astype('float64')\n\n   # Convert date columns to datetime\n   date_columns = ['disbursement_date', 'due_date']\n   df[date_columns] = df[date_columns].apply(pd.to_datetime)\n\n   # Convert categorical columns\n   df['loan_type'] = df['loan_type'].astype('category')\n   df['New_versus_Repeat'] = df['New_versus_Repeat'].astype('category')\n\n   # Convert target and duration to int\n   df['target'] = df['target'].astype('int8')  # Using int8 since it's binary\n   df['duration'] = df['duration'].astype('int32')\n\n   return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:18.764138Z","iopub.status.idle":"2025-01-11T12:26:18.764519Z","shell.execute_reply":"2025-01-11T12:26:18.764348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = convert_datatypes(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:18.766298Z","iopub.status.idle":"2025-01-11T12:26:18.766675Z","shell.execute_reply":"2025-01-11T12:26:18.766510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sorted_train_df = train_df.sort_values(by='disbursement_date')\nsorted_train_df['interest_and_fees'] = sorted_train_df['Total_Amount_to_Repay'] - sorted_train_df['Total_Amount']\nsorted_train_df['interest_rate'] = (interest_and_fees / Total_Amount) * 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:18.767515Z","iopub.status.idle":"2025-01-11T12:26:18.767902Z","shell.execute_reply":"2025-01-11T12:26:18.767718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sorted_train_df['Total_Amount']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:18.768632Z","iopub.status.idle":"2025-01-11T12:26:18.769036Z","shell.execute_reply":"2025-01-11T12:26:18.768875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.query(\"customer_id=='136048'\").sort_values(by='disbursement_date')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:18.769648Z","iopub.status.idle":"2025-01-11T12:26:18.770031Z","shell.execute_reply":"2025-01-11T12:26:18.769871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#","metadata":{}},{"cell_type":"markdown","source":"## playground","metadata":{}},{"cell_type":"code","source":"def create_aggregated_features(train_df, test_df=None):\n    \"\"\"\n    Creates essential aggregated features for credit scoring model.\n    \"\"\"\n    # Customer Behavior Features (most predictive for credit scoring)\n    customer_agg = train_df.groupby('customer_id').agg({\n        'tbl_loan_id': 'count',  # Number of loans - key indicator\n        'Total_Amount': ['mean', 'max'],  # Loan amount behavior\n        'duration': 'mean',  # Average loan duration\n        'New_versus_Repeat': lambda x: (x == 'Repeat').mean()  # Repeat customer ratio\n    }).round(3)\n    \n    # Flatten column names\n    customer_agg.columns = [\n        'loan_count',\n        'avg_loan_amount', \n        'max_loan_amount',\n        'avg_duration',\n        'repeat_loan_ratio'\n    ]\n    \n    # Essential Financial Ratios\n    def calculate_financial_ratios(df):\n        df = df.copy()\n        df['repayment_to_loan_ratio'] = df['Total_Amount_to_Repay'] / df['Total_Amount']\n        df['daily_repayment_amount'] = df['Total_Amount_to_Repay'] / df['duration']\n        return df\n    \n    # Apply transformations\n    if test_df is not None:\n        df_to_transform = test_df.copy()\n    else:\n        df_to_transform = train_df.copy()\n    \n    # Calculate ratios and merge aggregations\n    df_transformed = calculate_financial_ratios(df_to_transform)\n    df_transformed = df_transformed.merge(customer_agg, on='customer_id', how='left')\n    \n    # Handle missing values\n    df_transformed = df_transformed.fillna(0)\n    \n    # Core features for modeling\n    features_for_modeling = [\n        # Original features\n        'Total_Amount', \n        'Total_Amount_to_Repay', \n        'duration',\n        'Amount_Funded_By_Lender',\n        \n        # Key ratios\n        'repayment_to_loan_ratio',\n        'daily_repayment_amount',\n        \n        # Customer aggregations\n        'loan_count',\n        'avg_loan_amount',\n        'max_loan_amount',\n        'avg_duration',\n        'repeat_loan_ratio'\n    ]\n    \n    return df_transformed[features_for_modeling]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:18.925505Z","iopub.execute_input":"2025-01-11T12:26:18.925973Z","iopub.status.idle":"2025-01-11T12:26:18.935538Z","shell.execute_reply.started":"2025-01-11T12:26:18.925929Z","shell.execute_reply":"2025-01-11T12:26:18.934332Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def create_aggregated_features(train_df, test_df=None):\n    \"\"\"\n    Creates enhanced features for credit scoring with robust handling of numerical issues\n    \"\"\"\n    def safe_divide(a, b):\n        \"\"\"Safe division handling zeros and infinities\"\"\"\n        result = np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n        return np.clip(result, -1e15, 1e15)  # Clip extreme values\n    \n    # Customer Behavior and Risk Features\n    customer_agg = train_df.groupby('customer_id').agg({\n        'tbl_loan_id': 'count',\n        'Total_Amount': ['mean', 'max', 'sum', 'std'],\n        'duration': ['mean', 'max', 'std'],\n        'New_versus_Repeat': lambda x: (x == 'Repeat').mean(),\n        'Total_Amount_to_Repay': ['mean', 'sum'],\n        'Amount_Funded_By_Lender': ['mean', 'sum']\n    }).round(3)\n    \n    # Flatten column names\n    customer_agg.columns = [\n        'loan_count',\n        'avg_loan_amount', 'max_loan_amount', 'total_loan_amount', 'std_loan_amount',\n        'avg_duration', 'max_duration', 'std_duration',\n        'repeat_loan_ratio',\n        'avg_repayment_amount', 'total_repayment_burden',\n        'avg_funded_amount', 'total_funded_amount'\n    ]\n    \n    # Advanced Financial Ratios with safe division\n    def calculate_financial_ratios(df):\n        df = df.copy()\n        \n        # Basic ratios with safe division\n        df['repayment_to_loan_ratio'] = safe_divide(df['Total_Amount_to_Repay'], df['Total_Amount'])\n        df['daily_repayment_amount'] = safe_divide(df['Total_Amount_to_Repay'], df['duration'])\n        df['funded_ratio'] = safe_divide(df['Amount_Funded_By_Lender'], df['Total_Amount'])\n        \n        # Risk indicators\n        df['repayment_burden'] = safe_divide(df['Total_Amount_to_Repay'], df['duration'])\n        df['funding_gap'] = df['Total_Amount'] - df['Amount_Funded_By_Lender']\n        \n        # Interest calculations\n        df['interest_burden'] = df['Total_Amount_to_Repay'] - df['Total_Amount']\n        df['interest_rate'] = safe_divide(df['interest_burden'], df['Total_Amount'])\n        \n        return df\n    \n    # Lender Risk Metrics\n    lender_agg = train_df.groupby('lender_id').agg({\n        'Total_Amount': ['mean', 'std'],\n        'tbl_loan_id': 'count'\n    }).round(3)\n    \n    lender_agg.columns = [\n        'lender_avg_loan', 'lender_std_loan', 'lender_loan_count'\n    ]\n    \n    # Apply transformations\n    if test_df is not None:\n        df_to_transform = test_df.copy()\n    else:\n        df_to_transform = train_df.copy()\n    \n    # Calculate and merge all features\n    df_transformed = calculate_financial_ratios(df_to_transform)\n    df_transformed = df_transformed.merge(customer_agg, on='customer_id', how='left')\n    df_transformed = df_transformed.merge(lender_agg, on='lender_id', how='left')\n    \n    # Handle missing values and infinities\n    numeric_cols = df_transformed.select_dtypes(include=['float64', 'int64']).columns\n    df_transformed[numeric_cols] = df_transformed[numeric_cols].replace([np.inf, -np.inf], np.nan)\n    \n    # Fill missing values with median instead of mean (more robust to outliers)\n    medians = df_transformed[numeric_cols].median()\n    df_transformed[numeric_cols] = df_transformed[numeric_cols].fillna(medians)\n    \n    # Core features for modeling\n    features_for_modeling = [\n        # Original features\n        'Total_Amount', 'Total_Amount_to_Repay', 'duration',\n        'Amount_Funded_By_Lender',\n        \n        # Financial ratios\n        'repayment_to_loan_ratio', 'daily_repayment_amount',\n        'funded_ratio', 'repayment_burden', 'funding_gap',\n        'interest_burden', 'interest_rate',\n        \n        # Customer history\n        'loan_count', 'avg_loan_amount', 'max_loan_amount',\n        'total_loan_amount', 'std_loan_amount', 'avg_duration',\n        'max_duration', 'std_duration', 'repeat_loan_ratio',\n        'avg_repayment_amount', 'total_repayment_burden',\n        'avg_funded_amount', 'total_funded_amount',\n        \n        # Lender metrics\n        'lender_avg_loan', 'lender_std_loan', 'lender_loan_count'\n    ]\n    \n    # Final check for any remaining infinities\n    df_final = df_transformed[features_for_modeling]\n    df_final = df_final.clip(-1e15, 1e15)  # Clip any extreme values\n    \n    return df_final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:18.937091Z","iopub.execute_input":"2025-01-11T12:26:18.937444Z","iopub.status.idle":"2025-01-11T12:26:18.958215Z","shell.execute_reply.started":"2025-01-11T12:26:18.937413Z","shell.execute_reply":"2025-01-11T12:26:18.957365Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def prepare_economic_data(econ_df):\n    \"\"\"\n    Prepare economic data by melting year columns\n    \"\"\"\n    # List of indicators\n    INDICATORS = [\n        'Inflation, consumer prices (annual %)',\n        'Official exchange rate (LCU per US$, period average)',\n        'Real interest rate (%)',\n        'Average precipitation in depth (mm per year)',\n        'Deposit interest rate (%)',\n        'Lending interest rate (%)',\n        'Interest rate spread (lending rate minus deposit rate, %)',\n        'Fossil fuel energy consumption (% of total)',\n        'Unemployment rate'\n    ]\n    \n    # Keep original index and indicator names\n    econ_df = econ_df.copy()\n    \n    # Identify year columns\n    year_cols = [col for col in econ_df.columns if col.startswith('YR')]\n    \n    # Melt the dataframe\n    melted_df = pd.melt(\n        econ_df,\n        id_vars=['Country', 'Indicator'],  # Changed from CountryIndicator\n        value_vars=year_cols,\n        var_name='year',\n        value_name='value'\n    )\n    \n    # Clean up year column\n    melted_df['year'] = melted_df['year'].str.replace('YR', '').astype(int)\n    \n    # Create final economic indicators dataframe\n    final_df = pd.pivot_table(\n        melted_df,\n        index=['Country', 'year'],  # Changed from CountryIndicator\n        columns='Indicator',\n        values='value'\n    ).reset_index()\n    \n    # Rename columns for easier handling\n    column_mapping = {\n        'Country': 'country_id',  # Map Country to country_id\n        'Inflation, consumer prices (annual %)': 'inflation_rate',\n        'Official exchange rate (LCU per US$, period average)': 'exchange_rate',\n        'Real interest rate (%)': 'real_interest_rate',\n        'Average precipitation in depth (mm per year)': 'precipitation',\n        'Deposit interest rate (%)': 'deposit_rate',\n        'Lending interest rate (%)': 'lending_rate',\n        'Interest rate spread (lending rate minus deposit rate, %)': 'interest_spread',\n        'Fossil fuel energy consumption (% of total)': 'fossil_fuel_consumption',\n        'Unemployment rate': 'unemployment_rate'\n    }\n    \n    final_df = final_df.rename(columns=column_mapping)\n    \n    # Handle missing values\n    numerical_cols = list(column_mapping.values())[1:]  # Exclude country_id\n    final_df[numerical_cols] = final_df[numerical_cols].fillna(final_df[numerical_cols].mean())\n    \n    return final_df\n\n# Let's check unique countries in both datasets\nprint(\"Economic Data Countries:\", econ_in_df['Country'].unique())\nprint(\"Train Data Countries:\", train_df['country_id'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:18.960260Z","iopub.execute_input":"2025-01-11T12:26:18.960525Z","iopub.status.idle":"2025-01-11T12:26:19.047452Z","shell.execute_reply.started":"2025-01-11T12:26:18.960505Z","shell.execute_reply":"2025-01-11T12:26:19.046008Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-6ae057057e7a>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Let's check unique countries in both datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Economic Data Countries:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mecon_in_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Data Countries:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'econ_in_df' is not defined"],"ename":"NameError","evalue":"name 'econ_in_df' is not defined","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"def create_aggregated_features(train_df, test_df=None, econ_df=None):\n    \"\"\"\n    Creates features combining loan data and economic indicators\n    \"\"\"\n    def safe_divide(a, b):\n        \"\"\"Safe division handling zeros\"\"\"\n        return np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n    \n    # Customer Behavior Features\n    customer_agg = train_df.groupby('customer_id').agg({\n        'tbl_loan_id': ['count', 'nunique'],\n        'Total_Amount': ['mean', 'sum'],\n        'duration': ['mean', 'max', 'std'],\n        'Total_Amount_to_Repay': 'mean',\n        'New_versus_Repeat': lambda x: (x == 'Repeat').mean()\n    }).round(3)\n    \n    # Flatten column names\n    customer_agg.columns = [\n        'loan_count', 'unique_loans',\n        'avg_loan_amount', 'total_borrowed',\n        'avg_duration', 'max_duration', 'duration_std',\n        'avg_repayment_amount',\n        'repeat_ratio'\n    ]\n    \n    # Financial Ratios\n    def calculate_financial_ratios(df):\n        df = df.copy()\n        \n        # Core ratios (highest importance from previous analysis)\n        df['repayment_to_loan_ratio'] = safe_divide(df['Total_Amount_to_Repay'], df['Total_Amount'])\n        df['daily_repayment'] = safe_divide(df['Total_Amount_to_Repay'], df['duration'])\n        df['daily_principal'] = safe_divide(df['Total_Amount'], df['duration'])\n        \n        # Interest metrics\n        df['interest_amount'] = df['Total_Amount_to_Repay'] - df['Total_Amount']\n        df['interest_rate'] = safe_divide(df['interest_amount'], df['Total_Amount'])\n        df['daily_interest'] = safe_divide(df['interest_amount'], df['duration'])\n        \n        # Funding metrics\n        df['funding_ratio'] = safe_divide(df['Amount_Funded_By_Lender'], df['Total_Amount'])\n        df['funding_gap_ratio'] = 1 - df['funding_ratio']\n        \n        return df\n    \n    # Process data\n    if test_df is not None:\n        df_to_transform = test_df.copy()\n    else:\n        df_to_transform = train_df.copy()\n    \n    # Calculate features and merge aggregations\n    df_transformed = calculate_financial_ratios(df_to_transform)\n    df_transformed = df_transformed.merge(customer_agg, on='customer_id', how='left')\n    \n    # Add economic indicators\n    if econ_df is not None:\n        # Extract year from disbursement date\n        df_transformed['year'] = pd.to_datetime(df_transformed['disbursement_date']).dt.year\n        \n        # Merge with economic data\n        df_transformed = df_transformed.merge(\n            econ_df,\n            on=['country_id', 'year'],\n            how='left'\n        )\n        \n        # Create relative metrics using economic indicators\n        df_transformed['interest_to_market_ratio'] = safe_divide(df_transformed['interest_rate'], \n                                                               df_transformed['lending_rate'])\n        df_transformed['relative_funding_cost'] = df_transformed['interest_rate'] - df_transformed['lending_rate']\n    \n    # Handle missing values\n    numeric_cols = df_transformed.select_dtypes(include=['float64', 'int64']).columns\n    df_transformed[numeric_cols] = df_transformed[numeric_cols].replace([np.inf, -np.inf], np.nan)\n    medians = df_transformed[numeric_cols].median()\n    df_transformed[numeric_cols] = df_transformed[numeric_cols].fillna(medians)\n    \n    # Features for modeling\n    features_for_modeling = [\n        # Original features\n        'Total_Amount', 'duration',\n        'Amount_Funded_By_Lender',\n        \n        # High importance ratios\n        'repayment_to_loan_ratio',\n        'daily_repayment',\n        'daily_principal',\n        'daily_interest',\n        \n        # Customer metrics\n        'loan_count',\n        'unique_loans',\n        'avg_loan_amount',\n        'total_borrowed',\n        'avg_duration',\n        'max_duration',\n        'duration_std',\n        'repeat_ratio',\n        \n        # Risk indicators\n        'interest_rate',\n        'funding_ratio',\n        'funding_gap_ratio'\n    ]\n    \n    # Add economic features if available\n    if econ_df is not None:\n        economic_features = [\n            'inflation_rate',\n            'exchange_rate',\n            'real_interest_rate',\n            'deposit_rate',\n            'lending_rate',\n            'interest_spread',\n            'unemployment_rate',\n            'interest_to_market_ratio',\n            'relative_funding_cost'\n        ]\n        features_for_modeling.extend(economic_features)\n    \n    return df_transformed[features_for_modeling].clip(-1e15, 1e15)\n\n# Usage:\n# First prepare economic data\necon_df_prepared = prepare_economic_data(econ_in_df)\n\n# Then create features\nX = create_aggregated_features(train_df, econ_df=econ_df_prepared)\n\nprint(\"Final feature set shape:\", X.shape)\nprint(\"\\nFeatures included:\", X.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:19.048113Z","iopub.status.idle":"2025-01-11T12:26:19.048367Z","shell.execute_reply":"2025-01-11T12:26:19.048267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"viz = Visualizer()\nviz.plot_histograms(X, num_cols=3).show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:19.048970Z","iopub.status.idle":"2025-01-11T12:26:19.049385Z","shell.execute_reply":"2025-01-11T12:26:19.049192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-test split\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, train_df['target'], \n    stratify=train_df['target'],\n    test_size=0.2,\n    random_state=42\n)\n\n# Check final feature set\nprint(\"Final feature set shape:\", X.shape)\nprint(\"\\nFeatures included:\", X.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:19.050213Z","iopub.status.idle":"2025-01-11T12:26:19.050501Z","shell.execute_reply":"2025-01-11T12:26:19.050388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_focused_features(train_df, test_df=None):\n    \"\"\"\n    Creates simplified, focused features based on feature importance analysis\n    \"\"\"\n    def safe_divide(a, b):\n        \"\"\"Safe division handling zeros\"\"\"\n        return np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n    \n    # Create core features (common important features across models)\n    def calculate_core_features(df):\n        df = df.copy()\n        \n        # Most important features across all models\n        df['repayment_to_loan_ratio'] = safe_divide(df['Total_Amount_to_Repay'], df['Total_Amount'])\n        df['daily_interest'] = safe_divide(df['Total_Amount_to_Repay'] - df['Total_Amount'], df['duration'])\n        df['interest_rate'] = safe_divide(df['Total_Amount_to_Repay'] - df['Total_Amount'], df['Total_Amount'])\n        \n        return df\n    \n    # Basic customer aggregations\n    customer_agg = train_df.groupby('customer_id').agg({\n        'tbl_loan_id': 'count',          # Number of loans\n        'Total_Amount': ['mean', 'sum'],  # Borrowing behavior\n    }).round(3)\n    \n    # Flatten column names\n    customer_agg.columns = [\n        'loan_count',\n        'avg_loan_amount', \n        'total_borrowed'\n    ]\n    \n    # Process data\n    if test_df is not None:\n        df_to_transform = test_df.copy()\n    else:\n        df_to_transform = train_df.copy()\n    \n    # Calculate features and merge\n    df_transformed = calculate_core_features(df_to_transform)\n    df_transformed = df_transformed.merge(customer_agg, on='customer_id', how='left')\n    \n    # Handle missing values\n    df_transformed = df_transformed.fillna(0)\n    \n    # Final feature list\n    features_for_modeling = [\n        # Original features\n        'Total_Amount',\n        'duration',\n        \n        # Key ratios (most important across models)\n        'repayment_to_loan_ratio',\n        'daily_interest',\n        'interest_rate',\n        \n        # Basic customer metrics\n        'loan_count',\n        'avg_loan_amount',\n        'total_borrowed'\n    ]\n    \n    return df_transformed[features_for_modeling]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:19.051207Z","iopub.status.idle":"2025-01-11T12:26:19.051569Z","shell.execute_reply":"2025-01-11T12:26:19.051410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-test split\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, train_df['target'], \n    stratify=train_df['target'],\n    test_size=0.2,\n    random_state=42\n)\n\n# Check final feature set\nprint(\"Final feature set shape:\", X.shape)\nprint(\"\\nFeatures included:\", X.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:19.052336Z","iopub.status.idle":"2025-01-11T12:26:19.052618Z","shell.execute_reply":"2025-01-11T12:26:19.052516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import (\n    f1_score, classification_report, confusion_matrix,\n    roc_curve, precision_recall_curve, auc, roc_auc_score\n)\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.dummy import DummyClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_feature_importance(feature_importance, title):\n    \"\"\"\n    Plot feature importance as percentages with enhanced visualization\n    \"\"\"\n    # Calculate percentage importance\n    feature_importance['importance_pct'] = feature_importance['importance'] / feature_importance['importance'].sum() * 100\n    \n    plt.figure(figsize=(12, 6))\n    colors = sns.color_palette(\"viridis\", n_colors=10)\n    \n    ax = sns.barplot(\n        data=feature_importance.head(10),\n        x='importance_pct',\n        y='feature',\n        palette=colors\n    )\n    \n    plt.title(f'Top 10 Feature Importance - {title}', fontsize=12, pad=20)\n    plt.xlabel('Importance (%)')\n    plt.ylabel('Features')\n    \n    # Add percentage labels\n    for i, v in enumerate(feature_importance['importance_pct'].head(10)):\n        ax.text(v + 0.5, i, f'{v:.1f}%', va='center')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_validation_metrics(y_valid, valid_preds, valid_probs=None, model_name=\"\"):\n    \"\"\"\n    Comprehensive validation metrics visualization\n    \"\"\"\n    fig = plt.figure(figsize=(20, 5))\n    \n    # 1. Confusion Matrix\n    plt.subplot(1, 3, 1)\n    cm = confusion_matrix(y_valid, valid_preds)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=['No Default', 'Default'],\n                yticklabels=['No Default', 'Default'])\n    plt.title(f'Confusion Matrix - {model_name}')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    \n    if valid_probs is not None:\n        # 2. ROC Curve\n        plt.subplot(1, 3, 2)\n        fpr, tpr, _ = roc_curve(y_valid, valid_probs)\n        roc_auc = roc_auc_score(y_valid, valid_probs)\n        \n        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(f'ROC Curve - {model_name}')\n        plt.legend(loc=\"lower right\")\n        \n        # 3. Precision-Recall Curve\n        plt.subplot(1, 3, 3)\n        precision, recall, _ = precision_recall_curve(y_valid, valid_probs)\n        pr_auc = auc(recall, precision)\n        \n        plt.plot(recall, precision, label=f'PR curve (AUC = {pr_auc:.2f})')\n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.title(f'Precision-Recall Curve - {model_name}')\n        plt.legend(loc=\"lower left\")\n    \n    plt.tight_layout()\n    plt.show()\n\ndef train_evaluate_models(X_train, X_valid, y_train, y_valid):\n    \"\"\"\n    Train and evaluate multiple models with enhanced visualization\n    \"\"\"\n    # Calculate class weights\n    n_negative = len(y_train[y_train == 0])\n    n_positive = len(y_train[y_train == 1])\n    scale_pos_weight = n_negative / n_positive\n    \n    # Initialize scaler\n    scaler = RobustScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_valid_scaled = scaler.transform(X_valid)\n\n    # Initialize models with optimal parameters for credit scoring\n    models = {\n       'RandomForest': RandomForestClassifier(\n           n_estimators=200,\n           max_depth=7,\n           min_samples_split=10,\n           min_samples_leaf=4, \n           max_features='sqrt',\n           class_weight='balanced',\n           random_state=42,\n           n_jobs=-1\n       ),\n       \n       'XGBoost': xgb.XGBClassifier(\n           n_estimators=200,\n           max_depth=6,\n           learning_rate=0.1,\n           subsample=0.8,\n           colsample_bytree=0.8,\n           min_child_weight=3,\n           scale_pos_weight=scale_pos_weight,  # Calculated from data\n           random_state=42,\n           n_jobs=-1\n       ),\n       \n       'LightGBM': lgb.LGBMClassifier(\n           scale_pos_weight=scale_pos_weight,  # Calculated from data\n           random_state=42,\n           n_jobs=-1\n       )\n    }\n\n    \n    results = {}\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    for name, model in models.items():\n        print(f\"\\n{'='*50}\")\n        print(f\"Training {name}...\")\n        print('='*50)\n        \n        try:\n            # Cross-validation\n            cv_scores = cross_val_score(\n                model, X_train_scaled, y_train,\n                cv=cv, scoring='f1', n_jobs=-1\n            )\n            \n            # Model training\n            if name == 'XGBoost':\n                model.fit(\n                    X_train_scaled, y_train,\n                    eval_set=[(X_valid_scaled, y_valid)],\n                    early_stopping_rounds=50,\n                    verbose=False\n                )\n            else:\n                model.fit(X_train_scaled, y_train)\n            \n            # Predictions\n            train_preds = model.predict(X_train_scaled)\n            valid_preds = model.predict(X_valid_scaled)\n            valid_probs = model.predict_proba(X_valid_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n            \n            # Store results\n            results[name] = {\n                'cv_f1_mean': cv_scores.mean(),\n                'cv_f1_std': cv_scores.std(),\n                'train_f1': f1_score(y_train, train_preds),\n                'valid_f1': f1_score(y_valid, valid_preds),\n                'model': model,\n                'feature_importance': None\n            }\n            \n            # Print metrics\n            print(\"\\nModel Performance:\")\n            print(f\"CV F1 Score: {cv_scores.mean():.4f} (Â±{cv_scores.std()*2:.4f})\")\n            print(f\"Train F1 Score: {results[name]['train_f1']:.4f}\")\n            print(f\"Valid F1 Score: {results[name]['valid_f1']:.4f}\")\n            print(\"\\nClassification Report:\")\n            print(classification_report(y_valid, valid_preds))\n            \n            # Plot validation metrics\n            plot_validation_metrics(y_valid, valid_preds, valid_probs, name)\n            \n            # Feature importance\n            if hasattr(model, 'feature_importances_') and name != 'BaselineRandom':\n                feature_importance = pd.DataFrame({\n                    'feature': X_train.columns,\n                    'importance': model.feature_importances_\n                }).sort_values('importance', ascending=False)\n                results[name]['feature_importance'] = feature_importance\n                plot_feature_importance(feature_importance, name)\n            \n        except Exception as e:\n            print(f\"Error training {name}: {str(e)}\")\n            continue\n    \n    return results, scaler\n\n# Train and evaluate models\nresults, scaler = train_evaluate_models(X_train, X_valid, y_train, y_valid)\n\n# Find best model\nbest_model_name = max(results, key=lambda k: results[k]['valid_f1'])\nbest_model = results[best_model_name]['model']\n\nprint(\"\\nBest Model Summary:\")\nprint(\"=\"*50)\nprint(f\"Best Model: {best_model_name}\")\nprint(f\"Validation F1 Score: {results[best_model_name]['valid_f1']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:19.053199Z","iopub.status.idle":"2025-01-11T12:26:19.053515Z","shell.execute_reply":"2025-01-11T12:26:19.053398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import datetime\n\ndef create_submission(test_df, model, scaler, feature_columns):\n   \"\"\"\n   Create submission file for the competition\n   \"\"\"\n   econ_df_prepared = prepare_economic_data(econ_in_df)\n    \n   # Prepare test features using the same aggregation function used for training\n   X_test = create_aggregated_features(test_df, econ_df=econ_df_prepared)\n   \n   # Scale features using the same scaler fitted on training data\n   X_test_scaled = scaler.transform(X_test[feature_columns])\n   \n   # Make predictions\n   predictions = model.predict(X_test_scaled)\n   \n   # Create submission dataframe with required format\n   submission = pd.DataFrame({\n       'ID': test_df['ID'],\n       'Target': predictions.astype(int)  # Ensure predictions are integers (0 or 1)\n   })\n   \n   # Save submission to CSV\n   submission_filename = f'submission_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n   submission.to_csv(submission_filename, index=False)\n   \n   # Print summary\n   print(f\"\\nSubmission Summary:\")\n   print(\"-\" * 30)\n   print(f\"Total predictions: {len(predictions)}\")\n   print(f\"Predicted defaults (1): {sum(predictions == 1)}\")\n   print(f\"Predicted non-defaults (0): {sum(predictions == 0)}\")\n   print(f\"\\nSubmission saved as: {submission_filename}\")\n   \n   return submission\n\n\n# Create submission\nsubmission = create_submission(test_df, best_model, scaler, X_train.columns)\n\n# Verify submission format\nprint(\"\\nSubmission Format Verification:\")\nprint(\"-\" * 30)\nprint(\"Submission shape:\", submission.shape)\nprint(\"\\nFirst few rows:\")\nprint(submission.head())\nprint(\"\\nValue counts:\")\nprint(submission['Target'].value_counts())\n\n# Additional verification checks\nassert submission.shape[1] == 2, \"Submission should have exactly 2 columns\"\nassert submission.columns.tolist() == ['ID', 'Target'], \"Columns should be 'ID' and 'target'\"\nassert submission['Target'].isin([0, 1]).all(), \"Predictions should be binary (0 or 1)\"\nassert not submission.isnull().any().any(), \"Submission contains null values\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T12:26:19.105165Z","iopub.execute_input":"2025-01-11T12:26:19.105431Z","iopub.status.idle":"2025-01-11T12:26:19.118178Z","shell.execute_reply.started":"2025-01-11T12:26:19.105409Z","shell.execute_reply":"2025-01-11T12:26:19.116893Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-a6e6cd6be61a>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Create submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Verify submission format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"],"ename":"NameError","evalue":"name 'test_df' is not defined","output_type":"error"}],"execution_count":5}]}